{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Analysis of the WMJI Majic 105.7 Top 500 Songs of 1995"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract:** the results of an exploratory analysis of the \"Majic Number\" list of top 500 songs as found on the April 28, 1998 snapshot of the WMJI radio station website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toggle this to False if you need to fetch the\n",
    "# data for the first time\n",
    "useCache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The root web page from which to download the data set\n",
    "url = 'http://web.archive.org/web/19990428111902/http://www.wmji.com/fullsite/top500/top500.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeCache(listOfBeautifulSoups, timestamp, cachePrefix='',\n",
    "              cacheSuffix='.pickle' ):\n",
    "    '''writeCache(): ListOfBeautifulSoup datetime.datetime Str1 Str2 -> <File>\n",
    "    Purpose: to write the LisfOfBeautifulSoup to a cache file in pickle\n",
    "             format. datetime.datetime is the timestamp (in UTC) at \n",
    "             which the data were fetched, and is written into the name \n",
    "             of <file>. Optional keywords cachePrefix and cacheSuffix\n",
    "             set the start and end of the cache file's name.\n",
    "    '''\n",
    "    path = cachePrefix + str(datetime.utcnow()).replace(' ','T') + cacheSuffix\n",
    "    with open(path, 'wb') as rawCache:\n",
    "        defaultRecLimit = sys.getrecursionlimit()\n",
    "        sys.setrecursionlimit(10**6)\n",
    "        pickle.dump(listOfBeautifulSoups,rawCache)\n",
    "        sys.setrecursionlimit(defaultRecLimit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCache(path):\n",
    "    '''readCache(): [Str] --> ListOfSoups\n",
    "    Purpose: to read in the cached copy of listOfSoups from \n",
    "             its pickled cache file. Optional Str argument \"path\" \n",
    "             is alternative path to the cache file.\n",
    "    '''\n",
    "    with open(path,'rb') as rawCache:\n",
    "        listOfSoups = pickle.load(rawCache)\n",
    "    \n",
    "    return listOfSoups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findLatestCache(directory='./',cachePrefix='',\n",
    "                   cacheSuffix='.pickle'):\n",
    "    '''findLatestCache(): [Str1] [Str2]-> Str\n",
    "    Purpose: to find the latest cache file in the current working\n",
    "             directory or in the specified PathToDirectory, and \n",
    "             return the path of that file as a Str.\n",
    "    '''\n",
    "    caches = list()\n",
    "    \n",
    "    for dirpath, dirnames, filenames in os.walk(directory):\n",
    "        for file in filenames:\n",
    "            if cachePrefix in file and cacheSuffix in file:\n",
    "                caches.append(file)\n",
    "                \n",
    "    caches.sort()\n",
    "    \n",
    "    try:\n",
    "        return caches[-1]\n",
    "    except IndexError as e:\n",
    "        raise ValueError(\"No cache file found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if useCache:\n",
    "    listOfSoups = readCache(findLatestCache())\n",
    "else:\n",
    "    # Fetch the initial URL\n",
    "    response = requests.get(url)\n",
    "    # Parse the result and enumerate links to follow\n",
    "    rootSoup = BeautifulSoup(response.content.decode())\n",
    "    # Collect the URLs to the data set\n",
    "    links = [ link for link in rootSoup.find_all('a') if \"songsnumberpt\" in link['href'] ]\n",
    "    # Collect and parse the pages at each link\n",
    "    listOfSoups = list()\n",
    "\n",
    "    rootPage = url.split('/')\n",
    "    del rootPage[-1]\n",
    "    rootPage = '/'.join(rootPage)+'/'\n",
    "\n",
    "    fetchTime = datetime.utcnow()\n",
    "\n",
    "    for link in links:\n",
    "        thisLink = rootPage+link['href']\n",
    "        response = requests.get(thisLink)\n",
    "        soup = BeautifulSoup(response.content.decode())\n",
    "        listOfSoups.append(soup)\n",
    "    \n",
    "    writeCache(listOfSoups,fetchTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve data from the songs table on each page\n",
    "pages = list()\n",
    "for page in listOfSoups:\n",
    "    # There's only one table on each page\n",
    "    table = page.find_all('table')[0]\n",
    "    rows = list()\n",
    "    for element in table.contents:\n",
    "        if element == \"\\n\":\n",
    "            continue\n",
    "        elif element.find_all('th'):\n",
    "            headers = element.find_all('th')\n",
    "            fields = [header.text for header in headers]\n",
    "        else:\n",
    "            data = element.find_all('tr')\n",
    "            for row in data:\n",
    "                values = { datum.text for datum in row }\n",
    "                rows.append(values)\n",
    "    \n",
    "    pages.append({ 'fields' : fields,\n",
    "                   'values' : rows})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.NavigableString'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n"
     ]
    }
   ],
   "source": [
    "# LEFT OFF HERE. Something's amiss with parsing the table.\n",
    "# When you just table.contents, you get nice plaintext rows.\n",
    "# When you try to parse each row, you get three times the number\n",
    "# of rows you expect\n",
    "mypage = listOfSoups[0]\n",
    "table = mypage.find_all('table')[0]\n",
    "for item in table:\n",
    "    print(type(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(logicalRows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
